{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_result\n",
      "Pass           12361\n",
      "Withdrawn      10156\n",
      "Fail            7052\n",
      "Distinction     3024\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from time import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "column_names = \"code_module,code_presentation,id_student,gender,region,highest_education,imd_band,age_band,num_of_prev_attempts,studied_credits,disability,final_result\"\n",
    "dummies_col_names = \"gender_1,gender_2,gender_3,gender_0,disability_1,disability_2,disability_0,age_band_1,age_band_2,age_band_3,age_band_0,highest_education_1,highest_education_2,highest_education_3,highest_education_4,highest_education_5,highest_education_0,imd_band_1,imd_band_2,imd_band_3,imd_band_4,imd_band_5,imd_band_6,imd_band_7,imd_band_8,imd_band_9,imd_band_10,imd_band_0,code_presentation_1,code_presentation_2,code_presentation_3,code_presentation_4,code_presentation_0,code_module_1,code_module_2,code_module_3,code_module_4,code_module_5,code_module_6,code_module_7,code_module_0,region_1,region_2,region_3,region_4,region_15,region_6,region_7,region_8,region_9,region_10,region_11,region_12,region_13,region_14,region_0\"\n",
    "categorical_col_names = \"code_module,code_presentation,gender,region,highest_education,imd_band,age_band,disability\"\n",
    "\n",
    "column_names = column_names.split(',')\n",
    "dummies_col_names = dummies_col_names.split(',')\n",
    "categorical_col_names = categorical_col_names.split(',')\n",
    "\n",
    "\n",
    "df = pd.read_csv('studentInfo.csv')\n",
    "df = df.drop(['id_student'], axis=1)\n",
    "\n",
    "class_counts = df['final_result'].value_counts()\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA.ENCODER()\n",
    "# Open the JSON file\n",
    "with open('encoding.json', 'r') as f:\n",
    "        # Load the JSON file into a dictionary\n",
    "        mappings = json.load(f)\n",
    "\n",
    "# Replace NaN values with 'Unknown'\n",
    "for column_name in column_names:\n",
    "        if column_name in categorical_col_names:\n",
    "            df[column_name] = df[column_name].fillna(\"Unknown\")\n",
    "\n",
    "# Replace values in each column based on the mappings\n",
    "for column, mapping in mappings.items():\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE_RANDOMFOREST_MODEL()\n",
    "\n",
    "#df = df.sample(frac=0.2)\n",
    "\n",
    "grid = {\n",
    "            'n_estimators': [int(x) for x in range(100, 1100, 200)],\n",
    "            'max_depth': [int(x) for x in range(5, 20, 5)],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "model = GridSearchCV(estimator=rf, param_grid=grid, cv=3, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the given DataFrame.\n",
    "    Adds new features by combining existing features.\n",
    "\n",
    "    <Feature: num_of_prev_attempts + studied_credits>, \n",
    "    <Feature: num_of_prev_attempts / studied_credits>, \n",
    "    <Feature: studied_credits / num_of_prev_attempts>, \n",
    "    <Feature: num_of_prev_attempts * studied_credits>, \n",
    "    <Feature: num_of_prev_attempts - studied_credits>,\n",
    "    <Feature: num_of_prev_attempts ^ 2>,\n",
    "    <Feature: studied_credits ^ 2>,\n",
    "    <Feature: num_of_prev_attempts ^ 3>,\n",
    "    <Feature: studied_credits ^ 3>,\n",
    "    <Feature: log(num_of_prev_attempts)>,\n",
    "    <Feature: log(studied_credits)>\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to perform feature engineering on.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the new features added.\n",
    "    \"\"\"\n",
    "    #Feature: num_of_prev_attempts + studied_credits\n",
    "    df['num_of_prev_attempts + studied_credits'] = df['num_of_prev_attempts'] + df['studied_credits']\n",
    "\n",
    "    #Feature: num_of_prev_attempts / studied_credits\n",
    "    df['num_of_prev_attempts / studied_credits'] = np.where(df['studied_credits'] == 0, 0, df['num_of_prev_attempts'] / df['studied_credits'])\n",
    "\n",
    "    #Feature: studied_credits / num_of_prev_attempts\n",
    "    df['studied_credits / num_of_prev_attempts'] = np.where(df['num_of_prev_attempts'] == 0, 0, df['studied_credits'] / df['num_of_prev_attempts'])\n",
    "\n",
    "    #Feature: num_of_prev_attempts * studied_credits\n",
    "    df['num_of_prev_attempts * studied_credits'] = df['num_of_prev_attempts'] * df['studied_credits']\n",
    "\n",
    "    #Feature: num_of_prev_attempts - studied_credits\n",
    "    df['num_of_prev_attempts - studied_credits'] = df['num_of_prev_attempts'] - df['studied_credits']\n",
    "\n",
    "    #Feature: num_of_prev_attempts ^ 2\n",
    "    df['num_of_prev_attempts ^ 2'] = df['num_of_prev_attempts'] ** 2\n",
    "\n",
    "    #Feature: studied_credits ^ 2\n",
    "    df['studied_credits ^ 2'] = df['studied_credits'] ** 2\n",
    "\n",
    "    #Feature: num_of_prev_attempts ^ 3\n",
    "    df['num_of_prev_attempts ^ 3'] = df['num_of_prev_attempts'] ** 3\n",
    "\n",
    "    #Feature: studied_credits ^ 3\n",
    "    df['studied_credits ^ 3'] = df['studied_credits'] ** 3\n",
    "\n",
    "    #Feature: log(num_of_prev_attempts)\n",
    "    df['log(num_of_prev_attempts)'] = df['num_of_prev_attempts'].apply(lambda x: 0 if x == 0 else math.log(x))\n",
    "\n",
    "    #Feature: log(studied_credits)\n",
    "    df['log(studied_credits)'] = df['studied_credits'].apply(lambda x: 0 if x == 0 else math.log(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Dataset Head:\n",
      "   code_module  code_presentation  gender  region  highest_education  \\\n",
      "0            1                  2       1       2                  4   \n",
      "1            1                  2       2       1                  4   \n",
      "2            1                  2       2      15                  3   \n",
      "\n",
      "   imd_band  age_band  num_of_prev_attempts  studied_credits  disability  \\\n",
      "0        10         3                     0              240           1   \n",
      "1         3         2                     0               60           1   \n",
      "2         4         2                     0               60           2   \n",
      "\n",
      "   final_result  \n",
      "0             1  \n",
      "1             1  \n",
      "2             0  \n",
      "\n",
      "1/11-target droped from dataset\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_MODEL()\n",
    "\n",
    "time_start = time()\n",
    "print(\"0-Dataset Head:\")\n",
    "print(df.head(3))\n",
    "print()\n",
    "\n",
    "# Split the dataset into features and target\n",
    "# X has features minus the target column\n",
    "X = df.drop(['final_result'], axis=1)\n",
    "print(\"1/11-target droped from dataset\")\n",
    "\n",
    "if False:\n",
    "    # Feature Engineering\n",
    "    print(\"X columns: \", X.columns)\n",
    "    print(\"X head: \", X.head(3))\n",
    "    X = feature_engineering(X)\n",
    "    print(\"1.5-Feature Engineering applied to dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_oneHotEncoder_and_encode()\n",
    "\n",
    "def create_oneHotEncoder_and_encode(df):\n",
    "    \"\"\"\n",
    "    Create a OneHotEncoder and fit it to the given DataFrame and encode the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to fit the encoder to and to encode.\n",
    "    \n",
    "    Returns:\n",
    "    df_encoded (pandas.DataFrame): The DataFrame with the categorical columns encoded.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a OneHotEncoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Fit the encoder using the larger dataset\n",
    "    encoder.fit(df[categorical_col_names])\n",
    "\n",
    "    # Save the encoder to a file\n",
    "    #dump(encoder, 'OneHotEncoder.joblib')\n",
    "\n",
    "    # Transform the DataFrame and convert the sparse matrix to a dense array\n",
    "    df_encoded = pd.DataFrame(encoder.transform(df[categorical_col_names]).toarray())\n",
    "\n",
    "    # Convert the data type of the columns to int (ou bool?)\n",
    "    df_encoded = df_encoded.astype(bool)\n",
    "\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/11-encoder created and encode applied to dataset\n",
      "\n",
      "Class counts: \n",
      "final_result\n",
      "0    17208\n",
      "1    15385\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3/11-target column extracted from dataset into y\n",
      "4/11-dataset split into training and testing sets\n",
      "\n",
      "Head of X_train:\n",
      "          0      1      2      3      4      5      6      7     8      9   \\\n",
      "24791  False  False  False  False  False   True  False  False  True  False   \n",
      "3490   False   True  False  False  False  False  False  False  True  False   \n",
      "\n",
      "       ...     37     38     39     40     41     42     43     44    45  \\\n",
      "24791  ...  False  False  False  False  False   True  False  False  True   \n",
      "3490   ...  False  False  False  False  False  False   True  False  True   \n",
      "\n",
      "          46  \n",
      "24791  False  \n",
      "3490   False  \n",
      "\n",
      "[2 rows x 47 columns]\n",
      "\n",
      "Head of y_train:\n",
      "24791    1\n",
      "3490     1\n",
      "Name: final_result, dtype: int64\n",
      "\n",
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "5/11-model trained\n",
      "6/11-model prediction completed\n",
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "\n",
      "Cross-validated scores: [0.5681853  0.52799509 0.52676791 0.54065664 0.54081006]\n",
      "\n",
      "Previous score: 0.53\n",
      "Current F1 Score:  0.57\n",
      "\n",
      "Time to fit: 28.628753797213236 minutes\n",
      "Time elapsed: 164.32054127852123 minutes\n",
      "-train_model finished\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_MODEL() CONTINUES\n",
    "\n",
    "X = create_oneHotEncoder_and_encode(X)\n",
    "print(\"2/11-encoder created and encode applied to dataset\")\n",
    "\n",
    "class_counts = df['final_result'].value_counts()\n",
    "print()\n",
    "print(\"Class counts: \")\n",
    "print(class_counts)\n",
    "print()\n",
    "\n",
    "# y has the target column and is converted to binary\n",
    "y = df['final_result']\n",
    "print(\"3/11-target column extracted from dataset into y\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"4/11-dataset split into training and testing sets\")\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "\"\"\" sm = SMOTE()\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "print(\"4.1-SMOTE applied to training data\") \"\"\"\n",
    "\n",
    "print()\n",
    "print(\"Head of X_train:\")\n",
    "print(X_train.head(2))\n",
    "print()\n",
    "#print(\"X Dtypes\")\n",
    "#print(X_train.dtypes)\n",
    "#print()\n",
    "print(\"Head of y_train:\")\n",
    "print(y_train.head(2))\n",
    "print()\n",
    "#print(\"Y Dtypes\")\n",
    "#print(y_train.dtypes)\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(X_train, y_train)\n",
    "print(\"5/11-model trained\")\n",
    "time_fit = time()\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"6/11-model prediction completed\")\n",
    "\n",
    "# Save the model\n",
    "#database.update_model_file(database_name, model_id, model)\n",
    "#print(\"7/11-model file updated\")\n",
    "\n",
    "# Store the evaluation in the database\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "#database.store_evaluation(database_name, model_id, matrix)\n",
    "#print(\"8/11-evaluation stored in database\")\n",
    "\n",
    "# set model atribute is_trained to true\n",
    "#database.set_model_trained(database_name, model_id)\n",
    "#print(\"9/11-model trained set to true\")\n",
    "\n",
    "#if database.set_ds_train_id(database_name=database_name, model_id=model_id, ds_id=ds_id):\n",
    "    #print(\"10/11-dataset train id set\")\n",
    "        \n",
    "# Predict the probabilities for the test dataset\n",
    "\"\"\" y_score = model.predict_proba(X_test)[:, 1]\n",
    "vz.create_ROC(model_id, y_test, y_score)\n",
    "vz.create_confusion_matrix(database_name, model_id)\n",
    "vz.create_PRC(model_id, y_test, y_score)\n",
    "print(\"11/11-visualizations created\") \"\"\"\n",
    "\n",
    "# print f1 score\n",
    "tn, fp, fn, tp = matrix.ravel()\n",
    "    \n",
    "# Convert numpy.int64 types to int\n",
    "fp, fn, tp, tn = int(fp), int(fn), int(tp), int(tn)\n",
    "\n",
    "precision = tp / float(tp + fp)\n",
    "recall = tp / float(tp + fn)\n",
    "f1 = 2 * (precision * recall) / float(precision + recall)\n",
    "f1 = round(f1, 2)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print()\n",
    "print(\"Cross-validated scores:\", scores)\n",
    "\n",
    "print()\n",
    "print(\"Previous score: 0.53\")\n",
    "print(\"Current F1 Score: \", f1)\n",
    "print()\n",
    "\n",
    "time_end = time()\n",
    "print(f\"Time to fit: {(time_fit - time_start) / 60} minutes\")\n",
    "print(f\"Time elapsed: {(time_end - time_start) / 60} minutes\")\n",
    "print(\"-train_model finished\")\n",
    "\n",
    "# save the parameters in a file\n",
    "f1 = f1*100\n",
    "with open(f'best_parameters_rf_score{f1}.json', 'w') as f:\n",
    "    json.dump(model.best_params_, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
